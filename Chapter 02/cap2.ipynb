{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/names.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora/words.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25h\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n",
      "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
      "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
      "order to load all the package's dependencies. You can do this by selecting the\n",
      "'Restart kernel' or 'Restart runtime' option.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('popular')\n",
    "spacy.cli.download('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                text\n",
      "0                        Hello world\n",
      "1  Exploring NLP with spaCy and NLTK\n"
     ]
    }
   ],
   "source": [
    "data = {'text': ['Hello world', 'Exploring NLP with spaCy and NLTK']}\n",
    "df = pd.DataFrame(data)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spaCy Tokens: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n",
      "NLTK Tokens: ['Natural', 'Language', 'Processing', 'enables', 'computers', 'to', 'understand', 'human', 'language', '.']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Natural Language Processing enables computers to understand human language.\")\n",
    "spacy_tokens = [token.text for token in doc]\n",
    "nltk_tokens = nltk.word_tokenize(doc.text)\n",
    "print(\"spaCy Tokens:\", spacy_tokens)\n",
    "print(\"NLTK Tokens:\", nltk_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## I/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lorem ipsum dolor sit amet\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
      "Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi\n"
     ]
    }
   ],
   "source": [
    "with open('sample.txt', 'r') as file:\n",
    "    for line in file:\n",
    "        print(line.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'w') as file:\n",
    "    file.write(\"Hello, this is a test write operation.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('output.txt', 'a') as file:\n",
    "    file.write(\"Adding more text to the existing file.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Name   Surname        City\n",
      "0   John       Doe  Washington\n",
      "1  James      Bond      London\n",
      "2  Frank   Sinatra    New York\n",
      "3  Diego  Maradona      Naples\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "print(df)\n",
    "df.to_csv(\"modified_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOREM IPSUM DOLOR SIT AMET\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from itertools import islice\n",
    "chunk_size = 500 #number of lines\n",
    "with open('sample.txt', 'r') as file:\n",
    "    while True:\n",
    "        lines = list(islice(file, chunk_size))\n",
    "        if not lines:\n",
    "            break\n",
    "        print(lines[0].upper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('sample.txt', 'r') as infile, open('output.txt', 'w') as outfile:\n",
    "    for line in infile:\n",
    "        outfile.write(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "world!\n"
     ]
    }
   ],
   "source": [
    "## SLICING\n",
    "text = \"Hello, world!\"\n",
    "sliced_text = text[7:] #Slices from the 8th character to the end 3. \n",
    "print(sliced_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, world!\n"
     ]
    }
   ],
   "source": [
    "greeting = \"Hello\"\n",
    "target = \"world\"\n",
    "concatenated_text = greeting + \", \" + target + \"!\" \n",
    "print(concatenated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "python\n",
      "PYTHON\n"
     ]
    }
   ],
   "source": [
    "mixed_case = \"PyThOn\"\n",
    "print(mixed_case.lower())\n",
    "print(mixed_case.upper())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "surrounded by space\n"
     ]
    }
   ],
   "source": [
    "spaced_string = \" surrounded by space \"\n",
    "trimmed_string = spaced_string.strip()\n",
    "print(trimmed_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python find at index:  12\n"
     ]
    }
   ],
   "source": [
    "sentence = \"Looking for Python in this sentence.\"\n",
    "index = sentence.find(\"Python\")\n",
    "print(\"Python find at index: \", index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['apple', 'banana', 'cherry']\n"
     ]
    }
   ],
   "source": [
    "data = \"apple,banana,cherry\"\n",
    "fruits = data.split(',')\n",
    "print(fruits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam loves using Python\n"
     ]
    }
   ],
   "source": [
    "user = \"Adam\"\n",
    "language = \"Python\"\n",
    "formatted_string = \"{} loves using {}\".format(user, language) \n",
    "print(formatted_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adam loves using Python\n"
     ]
    }
   ],
   "source": [
    "f_string = f\"{user} loves using {language}\" \n",
    "print(f_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## REGEX page 13 errore\n",
    "!pip install regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found: Python\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "pattern = re.compile(r'\\b\\w+\\b')\n",
    "text = \"Python is great for NLP tasks.\"\n",
    "match = pattern.search(text)\n",
    "if match:\n",
    "    print(\"Found:\", match.group())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All matches: ['Python', 'is', 'great', 'for', 'NLP', 'tasks']\n"
     ]
    }
   ],
   "source": [
    "matches = pattern.findall(text)\n",
    "print(\"All matches:\", matches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parts: ['Python', 'is', 'great', 'for', 'NLP', 'tasks', '']\n",
      "Cleaned Text: Python is great for NLP tasks \n"
     ]
    }
   ],
   "source": [
    "split_pattern = re.compile(r'\\W+')\n",
    "parts = split_pattern.split(text)\n",
    "print(\"Parts:\", parts)\n",
    "Parts: ['Python', 'is', 'great', 'for', 'NLP', 'tasks', '']\n",
    "cleaned_text = re.sub(r'\\W+', ' ', text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word before 'great': is\n"
     ]
    }
   ],
   "source": [
    "complex_pattern = re.compile(r'\\b(\\w+)\\s+(?=great)')\n",
    "complex_match = complex_pattern.search(text)\n",
    "if complex_match:\n",
    "    print(\"Word before 'great':\", complex_match.group(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "## NLP libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Using cached thinc-8.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from spacy) (4.66.5)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from spacy) (24.1)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached blis-1.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Using cached MarkupSafe-3.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached marisa_trie-1.2.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.7 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.18.0)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached spacy-3.8.2-cp311-cp311-macosx_11_0_arm64.whl (6.3 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "Using cached thinc-8.3.2-cp311-cp311-macosx_11_0_arm64.whl (774 kB)\n",
      "Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.0.1-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached MarkupSafe-3.0.1-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Using cached marisa_trie-1.2.0-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, spacy-loggers, spacy-legacy, shellingham, pydantic-core, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic, preshed, markdown-it-py, language-data, jinja2, blis, rich, langcodes, confection, typer, thinc, weasel, spacy\n",
      "Successfully installed MarkupSafe-3.0.1 annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 certifi-2024.8.30 charset-normalizer-3.4.0 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 idna-3.10 jinja2-3.1.4 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.0 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-2.0.2 preshed-3.0.9 pydantic-2.9.2 pydantic-core-2.23.4 requests-2.32.3 rich-13.9.2 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 typer-0.12.5 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
      "Collecting gensim\n",
      "  Using cached gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.1 kB)\n",
      "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
      "  Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (114 kB)\n",
      "Collecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
      "  Using cached scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from gensim) (7.0.5)\n",
      "Requirement already satisfied: wrapt in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Using cached gensim-4.3.3-cp311-cp311-macosx_11_0_arm64.whl (24.0 MB)\n",
      "Using cached numpy-1.26.4-cp311-cp311-macosx_11_0_arm64.whl (14.0 MB)\n",
      "Using cached scipy-1.13.1-cp311-cp311-macosx_12_0_arm64.whl (30.3 MB)\n",
      "Installing collected packages: numpy, scipy, gensim\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.0.2\n",
      "    Uninstalling numpy-2.0.2:\n",
      "      Successfully uninstalled numpy-2.0.2\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "blis 1.0.1 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
      "thinc 8.3.2 requires numpy<2.1.0,>=2.0.0; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n",
      "Collecting textblob\n",
      "  Downloading textblob-0.18.0.post0-py3-none-any.whl.metadata (4.5 kB)\n",
      "Requirement already satisfied: nltk>=3.8 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from textblob) (3.9.1)\n",
      "Requirement already satisfied: click in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk>=3.8->textblob) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk>=3.8->textblob) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk>=3.8->textblob) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk>=3.8->textblob) (4.66.5)\n",
      "Downloading textblob-0.18.0.post0-py3-none-any.whl (626 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m626.3/626.3 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: textblob\n",
      "Successfully installed textblob-0.18.0.post0\n",
      "Collecting scipy==1.12\n",
      "  Downloading scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl.metadata (165 kB)\n",
      "Requirement already satisfied: numpy<1.29.0,>=1.22.4 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from scipy==1.12) (1.26.4)\n",
      "Downloading scipy-1.12.0-cp311-cp311-macosx_12_0_arm64.whl (31.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: scipy\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.13.1\n",
      "    Uninstalling scipy-1.13.1:\n",
      "      Successfully uninstalled scipy-1.13.1\n",
      "Successfully installed scipy-1.12.0\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!pip install gensim\n",
    "!pip install textblob\n",
    "!pip install scipy==1.12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words: ['Hello', 'world', '.', 'Welcome', 'to', 'NLP', 'using', 'Python', '.']\n",
      "Sentences: ['Hello world.', 'Welcome to NLP using Python.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincenzo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "text = \"Hello world. Welcome to NLP using Python.\" \n",
    "words = word_tokenize(text)\n",
    "sentences = sent_tokenize(text)\n",
    "print(\"Words:\", words)\n",
    "print(\"Sentences:\", sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "# pagina 17 riga 17 bisogna inserire il download di en_core_web_sm\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Part-of-Speech Tags: [('Apple', 'PROPN'), ('is', 'AUX'), ('looking', 'VERB'), ('at', 'ADP'), ('buying', 'VERB'), ('U.K.', 'PROPN'), ('startup', 'VERB'), ('for', 'ADP'), ('$', 'SYM'), ('1', 'NUM'), ('billion', 'NUM')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm') ## Errore, bisogna inserire prima il download !python -m spacy download en_core_web_sm\n",
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\") \n",
    "pos_tags = [(token.text, token.pos_) for token in doc] \n",
    "print(\"Part-of-Speech Tags:\", pos_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topics: [(0, '0.112*\"human\" + 0.112*\"eps\" + 0.111*\"user\"'), (1, '0.225*\"system\" + 0.157*\"user\" + 0.156*\"response\"'), (2, '0.233*\"interface\" + 0.136*\"human\" + 0.136*\"eps\"')]\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models\n",
    "texts = [['human', 'interface', 'computer'], ['survey', 'user', 'computer', 'system', 'response'], ['eps', 'user', 'interface', 'system'], ['system', 'human', 'system', 'eps'], ['user', 'response', 'time']]\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "ldamodel = models.ldamodel.LdaModel(corpus, num_topics=3, id2word=dictionary, passes=15)\n",
    "topics = ldamodel.print_topics(num_words=3) \n",
    "print(\"Topics:\", topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentiment: Sentiment(polarity=0.5, subjectivity=0.5535714285714286)\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "feedback = \"TextBlob is amazingly simple to use. What a great tool!\" \n",
    "blob = TextBlob(feedback)\n",
    "sentiment = blob.sentiment\n",
    "print(\"Sentiment:\", sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entities: [('Google', 'ORG'), ('Larry Page', 'PERSON'), ('Sergey Brin', 'PERSON')]\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Google was founded by Larry Page and Sergey Brin.\")\n",
    "entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "print(\"Entities:\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity: [(0, 0.99368715), (1, 0.28159213), (2, 0.99241674), (3, 0.31755772), (4, 0.31954414)]\n"
     ]
    }
   ],
   "source": [
    "from gensim import similarities\n",
    "index = similarities.MatrixSimilarity(ldamodel[corpus])\n",
    "query = \"user computer interaction\"\n",
    "query_bow = dictionary.doc2bow(query.lower().split())\n",
    "sims = index[ldamodel[query_bow]]\n",
    "print(\"Similarity:\", list(enumerate(sims)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errore mancano i commenti\n",
    "def clean_text(text):\n",
    "    #Remove punctuation\n",
    "    text = ''.join([char for char in text if char.isalnum() or char.isspace()])\n",
    "    #Convert to lowercase\n",
    "    text = text.lower()\n",
    "    #Strip whitespace\n",
    "    text = text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    text = clean_text(text)\n",
    "    tokens = text.split()\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_words(tokens):\n",
    "    word_count = {}\n",
    "    for token in tokens:\n",
    "        if token in word_count:\n",
    "            word_count[token] += 1\n",
    "        else:\n",
    "            word_count[token] = 1\n",
    "    return word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_word(text, search_term):\n",
    "    tokens = tokenize(text)\n",
    "    return search_term in tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentences_containing_word(text, word):\n",
    "    sentences = text.split('.')\n",
    "    return [sentence.strip() + '.' for sentence in sentences if word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    cleaned_text = clean_text(text)\n",
    "    tokens = tokenize(cleaned_text)\n",
    "    word_freq = count_words(tokens)\n",
    "    return word_freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# recipe 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: nltk in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (3.9.1)\n",
      "Requirement already satisfied: click in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk) (2024.9.11)\n",
      "Requirement already satisfied: tqdm in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from nltk) (4.66.5)\n",
      "Collecting schedule\n",
      "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: schedule\n",
      "Successfully installed schedule-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install nltk\n",
    "!pip install schedule ### errore, sul pdf c'è scritto schedulet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def read_and_clean(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df['cleaned_text'] = df['text'].apply(lambda x:x.strip().lower().replace(',', ''))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincenzo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/vincenzo/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "def analyze_text(df):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    df['tokens'] = df['cleaned_text'].apply(lambda x: [word for word in word_tokenize(x) if word not in stop_words])\n",
    "    df['word_count'] = df['tokens'].apply(len)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_data(df, output_path):\n",
    "    df.to_csv(output_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(file_path, output_path):\n",
    "    df = read_and_clean(file_path)\n",
    "    analyzed_df = analyze_text(df)\n",
    "    export_data(analyzed_df, output_path)\n",
    "if __name__ == \"__main__\":\n",
    "    main('input.csv', 'output.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[42], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m      8\u001b[0m     schedule\u001b[38;5;241m.\u001b[39mrun_pending()\n\u001b[0;32m----> 9\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import schedule\n",
    "import time\n",
    "def job():\n",
    "    print(\"Running scheduled job.\")\n",
    "    main('input.csv', 'output.csv')\n",
    "schedule.every().day.at(\"10:30\").do(job)\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### pagina 26 errore virgolette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15245.16s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ipdb\n",
      "  Downloading ipdb-0.13.13-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: ipython>=7.31.1 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipdb) (8.28.0)\n",
      "Requirement already satisfied: decorator in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipdb) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (0.1.7)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (3.0.48)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (2.18.0)\n",
      "Requirement already satisfied: stack-data in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5.13.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (5.14.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (4.12.2)\n",
      "Requirement already satisfied: pexpect>4.3 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from ipython>=7.31.1->ipdb) (4.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from jedi>=0.16->ipython>=7.31.1->ipdb) (0.8.4)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pexpect>4.3->ipython>=7.31.1->ipdb) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=7.31.1->ipdb) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.1.0)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from stack-data->ipython>=7.31.1->ipdb) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from stack-data->ipython>=7.31.1->ipdb) (0.2.3)\n",
      "Requirement already satisfied: six>=1.12.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from asttokens>=2.1.0->stack-data->ipython>=7.31.1->ipdb) (1.16.0)\n",
      "Downloading ipdb-0.13.13-py3-none-any.whl (12 kB)\n",
      "Installing collected packages: ipdb\n",
      "Successfully installed ipdb-0.13.13\n"
     ]
    }
   ],
   "source": [
    "!pip install ipdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errore virgolette sbagliate\n",
    "def process_text(text):\n",
    "    print(\"Original text:\", text)\n",
    "    cleaned_text = clean_text(text)\n",
    "    print(\"Cleaned text:\", cleaned_text)\n",
    "    tokens = tokenize(cleaned_text)\n",
    "    print(\"Tokens:\", tokens)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore virgolette sbagliate e indentazione sbagliata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.DEBUG, filename='debug.log', filemode='w',format='%(name)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# errore virgolette sbagliate\n",
    "def tokenize(text):\n",
    "    logging.debug(f\"Tokenizing text: {text}\")\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore virgolette"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numbers(text):\n",
    "    tokens = tokenize(text)\n",
    "    normalized_tokens = [token if not token.isdigit() else \"#NUM\" for token in tokens]\n",
    "    assert all(isinstance(token, str) for token in normalized_tokens), \"All tokens must be strings\"\n",
    "    return normalized_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipdb\n",
    "def complex_processing(text):\n",
    "    ipdb.set_trace()\n",
    "    processed_text = process_text(text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "..\n",
      "----------------------------------------------------------------------\n",
      "Ran 2 tests in 0.002s\n",
      "\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "import unittest\n",
    "class TestTextProcessing(unittest.TestCase):\n",
    "    def test_tokenize(self):\n",
    "        self.assertEqual(tokenize(\"hello world\"), [\"hello\", \"world\"])\n",
    "    def test_clean_text(self):\n",
    "        self.assertEqual(clean_text(\" Hello, World! \"), \"hello world\")\n",
    "if __name__ == '__main__':\n",
    "    unittest.main(argv=[''], exit=False) ## errore devo inserire questo argomento se eseguo su notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "     with open(file_path, encoding=\"utf-8\") as file: \n",
    "            return file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15914.54s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy>=1.23.2 (from pandas)\n",
      "  Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.2 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15924.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (2.1.2)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15929.98s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numba\n",
      "  Downloading numba-0.60.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.7 kB)\n",
      "Collecting llvmlite<0.44,>=0.43.0dev0 (from numba)\n",
      "  Downloading llvmlite-0.43.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.8 kB)\n",
      "Collecting numpy<2.1,>=1.22 (from numba)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Downloading numba-0.60.0-cp311-cp311-macosx_11_0_arm64.whl (2.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.6/2.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading llvmlite-0.43.0-cp311-cp311-macosx_11_0_arm64.whl (28.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m28.8/28.8 MB\u001b[0m \u001b[31m18.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Installing collected packages: numpy, llvmlite, numba\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.1.2\n",
      "    Uninstalling numpy-2.1.2:\n",
      "      Successfully uninstalled numpy-2.1.2\n",
      "Successfully installed llvmlite-0.43.0 numba-0.60.0 numpy-2.0.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install numba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "def load_data_in_chunks(file_path, chunk_size=1000):\n",
    "    reader = pd.read_csv(file_path, chunksize=chunk_size)\n",
    "    for chunk in reader:\n",
    "        print(chunk.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = {'Column1': range(10080), 'Column2': ['value'] * 10080}\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('large_dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RangeIndex(start=0, stop=1000, step=1)\n",
      "RangeIndex(start=1000, stop=2000, step=1)\n",
      "RangeIndex(start=2000, stop=3000, step=1)\n",
      "RangeIndex(start=3000, stop=4000, step=1)\n",
      "RangeIndex(start=4000, stop=5000, step=1)\n",
      "RangeIndex(start=5000, stop=6000, step=1)\n",
      "RangeIndex(start=6000, stop=7000, step=1)\n",
      "RangeIndex(start=7000, stop=8000, step=1)\n",
      "RangeIndex(start=8000, stop=9000, step=1)\n",
      "RangeIndex(start=9000, stop=10000, step=1)\n",
      "RangeIndex(start=10000, stop=10080, step=1)\n"
     ]
    }
   ],
   "source": [
    "load_data_in_chunks(\"large_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def vectorized_operations(data):\n",
    "    numbers = np.array(data)\n",
    "    return np.sqrt(numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numba import jit\n",
    "@jit\n",
    "def intensive_computation(data):\n",
    "    result = [x * x for x in range(100000)]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_text(text):\n",
    "    words = set(text.split())\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from functools import lru_cache\n",
    "@lru_cache(maxsize=100)\n",
    "def get_synonyms(word):\n",
    "    # Simulate an expensive operation\n",
    "    return expensive_lookup(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cProfile\n",
    "def main():\n",
    "    data = pd.read_csv('big_data.csv')\n",
    "    data['Name']=data['Name'].apply(lambda x:x.upper())\n",
    "    cProfile.run('main()')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16201.26s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip install requests ### errore manca la s nel pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def get_data(url):\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json(response):\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_with_params(url, params):\n",
    "    response = requests.get(url, params=params)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_data(url, data):\n",
    "    response = requests.post(url, json=data)\n",
    "    return response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_response(response):\n",
    "    if response.status_code == 200:\n",
    "        return response.json()\n",
    "    else:\n",
    "        response.raise_for_status()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def authenticated_request(url, headers, auth):\n",
    "    response = requests.get(url, headers=headers, auth=auth)\n",
    "    return check_response(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16361.40s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting requests\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Installing collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
      "Successfully installed certifi-2024.8.30 charset-normalizer-3.4.0 idna-3.10 requests-2.32.3 urllib3-2.2.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16367.48s - pydevd: Sending message related to process being replaced timed-out after 5 seconds\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (4.12.3)\n",
      "Requirement already satisfied: soupsieve>1.2 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from beautifulsoup4) (2.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install requests\n",
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "def fetch_page(url):\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "def parse_html(html_content):\n",
    "    soup = BeautifulSoup(html_content, 'html.parser')\n",
    "    return soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_elements(soup, tag, class_name):\n",
    "    elements = soup.find_all(tag, class_=class_name)\n",
    "    return elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    return text.replace('\\n', ' ').replace('\\r', '').strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_data(elements):\n",
    "    data_list = []\n",
    "    for element in elements:\n",
    "        cleaned_data = clean_text(element.get_text())\n",
    "        data_list.append(cleaned_data)\n",
    "        return data_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_website(base_url, total_pages): \n",
    "    all_data = [] \n",
    "    for i in range(1, total_pages + 1):\n",
    "        url = f\"{base_url}?page={i}\"\n",
    "        html_content = fetch_page(url)\n",
    "        soup = parse_html(html_content)\n",
    "        elements = extract_elements(soup, 'div', 'article-text')\n",
    "        page_data = compile_data(elements)\n",
    "        all_data.extend(page_data)\n",
    "        return all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
