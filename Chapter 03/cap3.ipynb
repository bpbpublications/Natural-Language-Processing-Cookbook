{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting unidecode\n",
      "  Downloading Unidecode-1.3.8-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading Unidecode-1.3.8-py3-none-any.whl (235 kB)\n",
      "Installing collected packages: unidecode\n",
      "Successfully installed unidecode-1.3.8\n",
      "Collecting textblob==0.15\n",
      "  Downloading textblob-0.15.0-py2.py3-none-any.whl.metadata (4.2 kB)\n",
      "Collecting nltk>=3.1 (from textblob==0.15)\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk>=3.1->textblob==0.15)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk>=3.1->textblob==0.15)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk>=3.1->textblob==0.15)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk>=3.1->textblob==0.15)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Downloading textblob-0.15.0-py2.py3-none-any.whl (631 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m631.8/631.8 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk, textblob\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 textblob-0.15.0 tqdm-4.66.5\n",
      "Collecting datetime\n",
      "  Downloading DateTime-5.5-py3-none-any.whl.metadata (33 kB)\n",
      "Collecting zope.interface (from datetime)\n",
      "  Downloading zope.interface-7.1.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (43 kB)\n",
      "Collecting pytz (from datetime)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Requirement already satisfied: setuptools in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from zope.interface->datetime) (65.5.0)\n",
      "Downloading DateTime-5.5-py3-none-any.whl (52 kB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Downloading zope.interface-7.1.0-cp311-cp311-macosx_11_0_arm64.whl (209 kB)\n",
      "Installing collected packages: pytz, zope.interface, datetime\n",
      "Successfully installed datetime-5.5 pytz-2024.2 zope.interface-7.1.0\n",
      "Collecting langdetect\n",
      "  Downloading langdetect-1.0.9.tar.gz (981 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m981.5/981.5 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n",
      "\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n",
      "\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: six in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from langdetect) (1.16.0)\n",
      "Building wheels for collected packages: langdetect\n",
      "  Building wheel for langdetect (pyproject.toml) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for langdetect: filename=langdetect-1.0.9-py3-none-any.whl size=993222 sha256=746e1569a53d7dcb5522a6ba7530bd460f52bf4d7fa2e141085a3fcbb91e139b\n",
      "  Stored in directory: /Users/vincenzo/Library/Caches/pip/wheels/0a/f2/b2/e5ca405801e05eb7c8ed5b3b4bcf1fcabcd6272c167640072e\n",
      "Successfully built langdetect\n",
      "Installing collected packages: langdetect\n",
      "Successfully installed langdetect-1.0.9\n"
     ]
    }
   ],
   "source": [
    "!pip install unidecode\n",
    "!pip install textblob==0.15\n",
    "!pip install datetime\n",
    "!pip install langdetect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import unidecode\n",
    "from textblob import TextBlob\n",
    "import re\n",
    "from datetime import datetime\n",
    "from langdetect import detect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_unicode(text):\n",
    "    return unidecode.unidecode(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_language(text):\n",
    "    blob = TextBlob(text)\n",
    "    if detect(text)!= 'en':\n",
    "        return str(blob.translate(from_lang=detect(text),to='en'))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_dates(text):\n",
    " # This regex pattern might need adjustments for specific formats\n",
    "    dates = re.findall(r'\\d{2}/\\d{2}/\\d{4}', text)\n",
    "    normalized_dates = [datetime.strptime(date,'%d/%m/%Y').strftime('%Y-%m-%d') for date in dates]\n",
    "    for date in set(dates):\n",
    "        text = text.replace(date, next(iter(normalized_dates)))\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_numbers(text):\n",
    "# Example normalization: convert all integers to 'number'\n",
    "    return re.sub(r'\\b\\d+\\b', 'number', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_synonyms(text, synonym_dict):\n",
    " # Example synonyms\n",
    "    for word, synonym in synonym_dict.items():\n",
    "        text = re.sub(r'\\b' + word + r'\\b', synonym, text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: He said, 'I am flying to N.Y. on 04/07/2023 and back on 12/07/2023. I can't wait!'\n",
      "Normalized Text: He said, 'I am flying to N.Y. on number-number-number and back on number-number-number. I cannot wait!'\n",
      "Original Text: Ciao, sono Mario e sono nato il 12/04/1990. Quest'anno saro' a N.Y per visitare la città\n",
      "Normalized Text: Hi, I'm Mario and I was born on number-number-number. This year I will be in New Yorkto visit the city\n"
     ]
    }
   ],
   "source": [
    "original_text_english = \"He said, 'I am flying to N.Y. on 04/07/2023 and back on 12/07/2023. I can't wait!'\"\n",
    "original_text_italian=\"Ciao, sono Mario e sono nato il 12/04/1990. Quest'anno saro' a N.Y per visitare la città\"\n",
    "synonyms = {'N.Y.': 'New York', \"can't\": 'cannot'}\n",
    "list_text=[original_text_english,original_text_italian]\n",
    "# Applying normalization functions\n",
    "for original_text in list_text:\n",
    "    normalized_text = normalize_unicode(original_text)\n",
    "    normalized_text = normalize_language(normalized_text)\n",
    "    normalized_text = normalize_dates(normalized_text)\n",
    "    normalized_text = normalize_numbers(normalized_text)\n",
    "    normalized_text = replace_synonyms(normalized_text, synonyms)\n",
    "    print(\"Original Text:\", original_text)\n",
    "    print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore beautiful soup pagina 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting beautifulsoup4\n",
      "  Using cached beautifulsoup4-4.12.3-py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting soupsieve>1.2 (from beautifulsoup4)\n",
      "  Using cached soupsieve-2.6-py3-none-any.whl.metadata (4.6 kB)\n",
      "Using cached beautifulsoup4-4.12.3-py3-none-any.whl (147 kB)\n",
      "Using cached soupsieve-2.6-py3-none-any.whl (36 kB)\n",
      "Installing collected packages: soupsieve, beautifulsoup4\n",
      "Successfully installed beautifulsoup4-4.12.3 soupsieve-2.6\n"
     ]
    }
   ],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_nested_html(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    for script in soup([\"script\", \"style\"]): # remove all script and style tags\n",
    "        script.extract()\n",
    "    return soup.get_text(separator=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preserve_html_entities(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.decode(formatter=\"html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_spacing(text):\n",
    "    clean_text = ' '.join(text.split())\n",
    "    return clean_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def quality_check(text, original_text):\n",
    "    if len(text) >= len(original_text):\n",
    "        return False # indicates potential data loss or incorrect removal\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quality check result: True\n",
      "Original HTML Content: <html><head><title>Test</title></head><body><h1>Heading</h1><p>This is a <b>bold</b> paragraph.</p><script>console.log('ignore this');</script></body></html>\n",
      "Cleaned Text: TestHeadingThis is a bold paragraph.\n"
     ]
    }
   ],
   "source": [
    "sample_html =\"<html><head><title>Test</title></head><body><h1>Heading</h1><p>This is a <b>bold</b> paragraph.</p><script>console.log('ignore this');</script></body></html>\"\n",
    "clean_text = remove_html_tags(sample_html)\n",
    "clean_text= clean_nested_html(clean_text)\n",
    "clean_text=preserve_html_entities(clean_text)\n",
    "clean_text = adjust_spacing(clean_text)\n",
    "print(\"Quality check result:\",quality_check(text=clean_text,original_text=sample_html))\n",
    "print(\"Original HTML Content:\", sample_html)\n",
    "print(\"Cleaned Text:\", clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pyspellchecker\n",
      "  Downloading pyspellchecker-0.8.1-py3-none-any.whl.metadata (9.4 kB)\n",
      "Downloading pyspellchecker-0.8.1-py3-none-any.whl (6.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.8/6.8 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pyspellchecker\n",
      "Successfully installed pyspellchecker-0.8.1\n"
     ]
    }
   ],
   "source": [
    "!pip install pyspellchecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_spell_checker():\n",
    "    spell = SpellChecker()\n",
    "    return spell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    return text.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_misspelled_words(text, spell):\n",
    "    tokens = tokenize(text)\n",
    "    misspelled = spell.unknown(tokens)\n",
    "    return misspelled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_words(text, spell):\n",
    "    misspelled = find_misspelled_words(text, spell)\n",
    "    corrected_text = text\n",
    "    for word in misspelled:\n",
    " # Get the one `most likely` answer\n",
    "        corrected_word = spell.correction(word)\n",
    "        corrected_text = corrected_text.replace(word, corrected_word)\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contextual_correction(text, spell):\n",
    "    corrected_text = correct_words(text, spell)\n",
    "    # Additional context-based improvements can be coded here\n",
    "    return corrected_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text: I ave receved an invitaion to an excusive event.\n",
      "Corrected Text: I have received an invitation to an exclusive event\n"
     ]
    }
   ],
   "source": [
    "original_text = \"I ave receved an invitaion to an excusive event.\"\n",
    "spell = initialize_spell_checker()\n",
    "corrected_text = apply_contextual_correction(original_text, spell)\n",
    "print(\"Original Text:\", original_text)\n",
    "print(\"Corrected Text:\", corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore pagina 12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Could not find a version that satisfies the requirement re (from versions: none)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for re\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_punctuation():\n",
    "    return re.compile(r'[\\p{P}]+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text, pattern):\n",
    "    return pattern.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unicode_punctuation(text):  \n",
    "    unicode_punctuation = re.compile(r'[\\p{S}\\p{P}]+', re.UNICODE)\n",
    "    return unicode_punctuation.sub('', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    punctuation_pattern = define_punctuation()\n",
    "    text = remove_punctuation(text, punctuation_pattern)\n",
    "    text = remove_unicode_punctuation(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_text_integrity(original, cleaned):\n",
    "    if not cleaned.isalnum() and not cleaned:\n",
    "        raise ValueError(\"Text integrity compromised: non-alphanumeric characters detected.\")\n",
    "    if len(cleaned)>=len(original):\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text integrity passed? True\n",
      "Original Text: Hello, world! Welcome to the universe of NLP; where processing... becomes easy?\n",
      "Cleaned Text: Hello world Welcome to the universe of NLP where processing becomes easy\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Hello, world! Welcome to the universe of NLP; where processing... becomes easy?\"\n",
    "cleaned_text = clean_text(sample_text)\n",
    "print(\"Text integrity passed?\",verify_text_integrity(sample_text,cleaned_text))\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Cleaned Text:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(text):\n",
    "    return text.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_case(text, exceptions):\n",
    "    words = text.split()\n",
    "    normalized_words = [to_lowercase(word) if word.lower() not in exceptions else word for word in words]\n",
    "    return ' '.join(normalized_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_normalization(text, exceptions:set=set()):\n",
    "    return normalize_case(text, exceptions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_normalization(text,exceptions:set=None):\n",
    "    if exceptions:\n",
    "        text =str(set(text.split(' '))-exceptions)\n",
    "    if text != to_lowercase(text):\n",
    "        return False # Indicates not all text is in lowercase\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check normalization: True\n",
      "Original Text: This is an EXAMPLE of Mixed CASE Text.\n",
      "Normalized Text: this is an example of mixed case text.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"This is an EXAMPLE of Mixed CASE Text.\"\n",
    "exceptions = {'EXAMPLE'}\n",
    "normalized_text = apply_normalization(sample_text, exceptions)\n",
    "print(\"Check normalization:\", check_normalization(normalized_text,exceptions=exceptions))\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import regex as re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_whitespace_patterns():\n",
    "    return re.compile(r'\\s+')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_excess_whitespace(text):\n",
    "    pattern = define_whitespace_patterns()\n",
    "    return pattern.sub(' ', text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_special_whitespace(text):\n",
    "    # Replace tabs and newlines explicitly if required\n",
    "    text = text.replace('\\t', ' ').replace('\\n', ' ')\n",
    "    return remove_excess_whitespace(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_text_structure(text):\n",
    "    if re.search(r'^\\s|\\s$', text):\n",
    "        raise ValueError(\"Text structure compromised: leading or trailing whitespace detected.\")\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_whitespace_normalization(text):\n",
    "    cleaned_text = remove_special_whitespace(text)\n",
    "    cleaned_text = verify_text_structure(cleaned_text)\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:  This text\t has \n",
      " excessive \n",
      "\n",
      "whitespace.\n",
      " \n",
      "Normalized Text: This text has excessive whitespace.\n"
     ]
    }
   ],
   "source": [
    "sample_text = \" This text\\t has \\n excessive \\n\\nwhitespace.\\n \"\n",
    "normalized_text = apply_whitespace_normalization(sample_text)\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Normalized Text:\", normalized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincenzo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def segment_sentences(text):\n",
    "    return sent_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_special_cases(sentences):\n",
    "    adjusted_sentences = []\n",
    "    for sentence in sentences:\n",
    "        if sentence.endswith(('Mr.', 'Mrs.', 'Dr.', 'Inc.')): # Add more cases as needed\n",
    "            next_index = sentences.index(sentence) + 1\n",
    "            if next_index < len(sentences):\n",
    "                combined_sentence = sentence + ' ' + sentences[next_index]\n",
    "                adjusted_sentences.append(combined_sentence)\n",
    "            else:\n",
    "                adjusted_sentences.append(sentence)\n",
    "        else:\n",
    "            if not adjusted_sentences or not adjusted_sentences[-1].endswith(('Mr.', 'Mrs.', 'Dr.', 'Inc.')):\n",
    "                adjusted_sentences.append(sentence)\n",
    "    return adjusted_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_segmentation(text):\n",
    "    sentences = segment_sentences(text)\n",
    "    sentences = handle_special_cases(sentences)\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmented Sentences: ['Dr. Smith loves New York.', 'He moved there in 2010.', 'He founded an Hi-Tec company ACME Inc. which sells computers.', \"He's married with Mrs. Laura Bennet.\", \"Isn't it great?\", 'Yes, he said so.']\n"
     ]
    }
   ],
   "source": [
    "text_example = \"Dr. Smith loves New York. He moved there in 2010. He founded an Hi-Tec company ACME Inc. which sells computers. He's married with Mrs. Laura Bennet. Isn't it great? Yes, he said so.\"\n",
    "segmented_sentences = validate_segmentation(text_example)\n",
    "print(\"Segmented Sentences:\", segmented_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Installing collected packages: numpy\n",
      "Successfully installed numpy-2.1.2\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl.metadata (13 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.6.0 (from scikit-learn)\n",
      "  Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.2-cp311-cp311-macosx_12_0_arm64.whl (11.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.0/11.0 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Downloading scipy-1.14.1-cp311-cp311-macosx_14_0_arm64.whl (23.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.1/23.1 MB\u001b[0m \u001b[31m11.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hDownloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.2 scipy-1.14.1 threadpoolctl-3.5.0\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy\n",
    "!pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vectorizer(vectorizer_type='count'):\n",
    "    if vectorizer_type == 'tfidf':\n",
    "        return TfidfVectorizer()\n",
    "    else:\n",
    "        return CountVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_encode(texts, vectorizer):\n",
    "    vectorizer.fit(texts)\n",
    "    return vectorizer.transform(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_vectors(vectors):\n",
    "    norm = np.linalg.norm(vectors.toarray(), axis=1, keepdims=True)\n",
    "    return vectors / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data_for_model(texts, vectorizer_type='count'):\n",
    "    vectorizer = initialize_vectorizer(vectorizer_type)\n",
    "    vectors = tokenize_and_encode(texts, vectorizer)\n",
    "    vectors = normalize_vectors(vectors)\n",
    "    return vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorized Data Shape: (2, 9)\n",
      "Vector Example\n",
      " <COOrdinate sparse matrix of dtype 'float64'\n",
      "\twith 9 stored elements and shape (2, 9)>\n",
      "  Coords\tValues\n",
      "  (0, 1)\t0.7071067811865476\n",
      "  (0, 8)\t0.7071067811865476\n",
      "  (1, 0)\t0.3779644730092273\n",
      "  (1, 2)\t0.3779644730092273\n",
      "  (1, 3)\t0.3779644730092273\n",
      "  (1, 4)\t0.3779644730092273\n",
      "  (1, 5)\t0.3779644730092273\n",
      "  (1, 6)\t0.3779644730092273\n",
      "  (1, 7)\t0.3779644730092273\n"
     ]
    }
   ],
   "source": [
    "sample_texts = [\"Hello world\", \" Vectorizing NLP text for Machine learning training\"]\n",
    "vectorized_data = prepare_data_for_model(sample_texts, 'tfidf')\n",
    "print(\"Vectorized Data Shape:\", vectorized_data.shape)\n",
    "print(\"Vector Example\\n\",vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl.metadata (40 kB)\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.66.5-py3-none-any.whl.metadata (57 kB)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached regex-2024.9.11-cp311-cp311-macosx_11_0_arm64.whl (284 kB)\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.66.5-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.9.11 tqdm-4.66.5\n",
      "Collecting spacy\n",
      "  Using cached spacy-3.8.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (27 kB)\n",
      "Collecting spacy-legacy<3.1.0,>=3.0.11 (from spacy)\n",
      "  Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl.metadata (2.8 kB)\n",
      "Collecting spacy-loggers<2.0.0,>=1.0.0 (from spacy)\n",
      "  Using cached spacy_loggers-1.0.5-py3-none-any.whl.metadata (23 kB)\n",
      "Collecting murmurhash<1.1.0,>=0.28.0 (from spacy)\n",
      "  Using cached murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.0 kB)\n",
      "Collecting cymem<2.1.0,>=2.0.2 (from spacy)\n",
      "  Using cached cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (8.4 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2 (from spacy)\n",
      "  Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl.metadata (2.2 kB)\n",
      "Collecting thinc<8.4.0,>=8.3.0 (from spacy)\n",
      "  Using cached thinc-8.3.2-cp311-cp311-macosx_11_0_arm64.whl.metadata (15 kB)\n",
      "Collecting wasabi<1.2.0,>=0.9.1 (from spacy)\n",
      "  Using cached wasabi-1.1.3-py3-none-any.whl.metadata (28 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.3 (from spacy)\n",
      "  Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6 (from spacy)\n",
      "  Using cached catalogue-2.0.10-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting weasel<0.5.0,>=0.1.0 (from spacy)\n",
      "  Using cached weasel-0.4.1-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting typer<1.0.0,>=0.3.0 (from spacy)\n",
      "  Using cached typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from spacy) (4.66.5)\n",
      "Collecting requests<3.0.0,>=2.13.0 (from spacy)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Collecting pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 (from spacy)\n",
      "  Using cached pydantic-2.9.2-py3-none-any.whl.metadata (149 kB)\n",
      "Collecting jinja2 (from spacy)\n",
      "  Using cached jinja2-3.1.4-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: setuptools in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from spacy) (65.5.0)\n",
      "Collecting packaging>=20.0 (from spacy)\n",
      "  Using cached packaging-24.1-py3-none-any.whl.metadata (3.2 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0 (from spacy)\n",
      "  Using cached langcodes-3.4.1-py3-none-any.whl.metadata (29 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Collecting language-data>=1.2 (from langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Using cached language_data-1.2.0-py3-none-any.whl.metadata (4.3 kB)\n",
      "Collecting annotated-types>=0.6.0 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached annotated_types-0.7.0-py3-none-any.whl.metadata (15 kB)\n",
      "Collecting pydantic-core==2.23.4 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting typing-extensions>=4.6.1 (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy)\n",
      "  Using cached typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (34 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached urllib3-2.2.3-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3.0.0,>=2.13.0->spacy)\n",
      "  Using cached certifi-2024.8.30-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting blis<1.1.0,>=1.0.0 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached blis-1.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (7.6 kB)\n",
      "Collecting confection<1.0.0,>=0.0.1 (from thinc<8.4.0,>=8.3.0->spacy)\n",
      "  Using cached confection-0.1.5-py3-none-any.whl.metadata (19 kB)\n",
      "Collecting numpy>=1.19.0 (from spacy)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: click>=8.0.0 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
      "Collecting shellingham>=1.3.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached shellingham-1.5.4-py2.py3-none-any.whl.metadata (3.5 kB)\n",
      "Collecting rich>=10.11.0 (from typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached rich-13.9.2-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting cloudpathlib<1.0.0,>=0.7.0 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached cloudpathlib-0.19.0-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting smart-open<8.0.0,>=5.2.1 (from weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached smart_open-7.0.5-py3-none-any.whl.metadata (24 kB)\n",
      "Collecting MarkupSafe>=2.0 (from jinja2->spacy)\n",
      "  Using cached MarkupSafe-3.0.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (4.0 kB)\n",
      "Collecting marisa-trie>=0.7.7 (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy)\n",
      "  Downloading marisa_trie-1.2.1-cp311-cp311-macosx_11_0_arm64.whl.metadata (9.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Collecting pygments<3.0.0,>=2.13.0 (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached pygments-2.18.0-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting wrapt (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy)\n",
      "  Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl.metadata (6.6 kB)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached spacy-3.8.2-cp311-cp311-macosx_11_0_arm64.whl (6.3 MB)\n",
      "Using cached catalogue-2.0.10-py3-none-any.whl (17 kB)\n",
      "Using cached cymem-2.0.8-cp311-cp311-macosx_11_0_arm64.whl (41 kB)\n",
      "Using cached langcodes-3.4.1-py3-none-any.whl (182 kB)\n",
      "Using cached murmurhash-1.0.10-cp311-cp311-macosx_11_0_arm64.whl (26 kB)\n",
      "Using cached packaging-24.1-py3-none-any.whl (53 kB)\n",
      "Using cached preshed-3.0.9-cp311-cp311-macosx_11_0_arm64.whl (128 kB)\n",
      "Using cached pydantic-2.9.2-py3-none-any.whl (434 kB)\n",
      "Using cached pydantic_core-2.23.4-cp311-cp311-macosx_11_0_arm64.whl (1.8 MB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached spacy_legacy-3.0.12-py2.py3-none-any.whl (29 kB)\n",
      "Using cached spacy_loggers-1.0.5-py3-none-any.whl (22 kB)\n",
      "Using cached srsly-2.4.8-cp311-cp311-macosx_11_0_arm64.whl (488 kB)\n",
      "Using cached thinc-8.3.2-cp311-cp311-macosx_11_0_arm64.whl (774 kB)\n",
      "Using cached numpy-2.0.2-cp311-cp311-macosx_14_0_arm64.whl (5.3 MB)\n",
      "Using cached typer-0.12.5-py3-none-any.whl (47 kB)\n",
      "Using cached wasabi-1.1.3-py3-none-any.whl (27 kB)\n",
      "Using cached weasel-0.4.1-py3-none-any.whl (50 kB)\n",
      "Using cached jinja2-3.1.4-py3-none-any.whl (133 kB)\n",
      "Using cached annotated_types-0.7.0-py3-none-any.whl (13 kB)\n",
      "Using cached blis-1.0.1-cp311-cp311-macosx_11_0_arm64.whl (1.1 MB)\n",
      "Using cached certifi-2024.8.30-py3-none-any.whl (167 kB)\n",
      "Using cached charset_normalizer-3.4.0-cp311-cp311-macosx_11_0_arm64.whl (118 kB)\n",
      "Using cached cloudpathlib-0.19.0-py3-none-any.whl (49 kB)\n",
      "Using cached confection-0.1.5-py3-none-any.whl (35 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached language_data-1.2.0-py3-none-any.whl (5.4 MB)\n",
      "Using cached MarkupSafe-3.0.1-cp311-cp311-macosx_11_0_arm64.whl (12 kB)\n",
      "Using cached rich-13.9.2-py3-none-any.whl (242 kB)\n",
      "Using cached shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
      "Using cached smart_open-7.0.5-py3-none-any.whl (61 kB)\n",
      "Using cached typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
      "Using cached urllib3-2.2.3-py3-none-any.whl (126 kB)\n",
      "Downloading marisa_trie-1.2.1-cp311-cp311-macosx_11_0_arm64.whl (174 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached pygments-2.18.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wrapt-1.16.0-cp311-cp311-macosx_11_0_arm64.whl (38 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: cymem, wrapt, wasabi, urllib3, typing-extensions, spacy-loggers, spacy-legacy, shellingham, pygments, packaging, numpy, murmurhash, mdurl, MarkupSafe, marisa-trie, idna, cloudpathlib, charset-normalizer, certifi, catalogue, annotated-types, srsly, smart-open, requests, pydantic-core, preshed, markdown-it-py, language-data, jinja2, blis, rich, pydantic, langcodes, typer, confection, weasel, thinc, spacy\n",
      "Successfully installed MarkupSafe-3.0.1 annotated-types-0.7.0 blis-1.0.1 catalogue-2.0.10 certifi-2024.8.30 charset-normalizer-3.4.0 cloudpathlib-0.19.0 confection-0.1.5 cymem-2.0.8 idna-3.10 jinja2-3.1.4 langcodes-3.4.1 language-data-1.2.0 marisa-trie-1.2.1 markdown-it-py-3.0.0 mdurl-0.1.2 murmurhash-1.0.10 numpy-2.0.2 packaging-24.1 preshed-3.0.9 pydantic-2.9.2 pydantic-core-2.23.4 pygments-2.18.0 requests-2.32.3 rich-13.9.2 shellingham-1.5.4 smart-open-7.0.5 spacy-3.8.2 spacy-legacy-3.0.12 spacy-loggers-1.0.5 srsly-2.4.8 thinc-8.3.2 typer-0.12.5 typing-extensions-4.12.2 urllib3-2.2.3 wasabi-1.1.3 weasel-0.4.1 wrapt-1.16.0\n",
      "Collecting en-core-web-md==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_md-3.8.0/en_core_web_md-3.8.0-py3-none-any.whl (33.5 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m33.5/33.5 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-md\n",
      "Successfully installed en-core-web-md-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_md')\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/vincenzo/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from nltk.tokenize import word_tokenize\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_tokenizer(text):\n",
    "# This tokenizer will need to handle specific punctuation and formats unique to the domain.\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def integrate_domain_vocabulary(text, vocabulary):\n",
    "    # Integrate domain-specific vocabulary into the tokenizer or text processing pipeline.\n",
    "    return \" \".join([vocabulary.get(word, word) for word in text.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_contextual_rules(text, rules):\n",
    "    # Apply domain-specific rules that adjust text based on its context.\n",
    "    for pattern, replacement in rules.items():\n",
    "        text = text.replace(pattern, replacement)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_semantics(text, synonym_dict):\n",
    "    words = text.split()\n",
    "    normalized_text = ' '.join([synonym_dict.get(word, word) for word in words])\n",
    "    return normalized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_entities(text, nlp_model):\n",
    "    nlp = spacy.load(nlp_model)\n",
    "    doc = nlp(text)\n",
    "    return [(ent.text, ent.label_) for ent in doc.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore pagina 24, controllare la formattazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Text: Dr. Watson reviewed the patient 's history before prescribing Paracetamol 500 mg daily and 5 mg of Oxycodone every 10 hours .\n",
      "Recognized Entities: [('Watson', 'PERSON'), ('5 mg', 'QUANTITY'), ('every 10 hours', 'TIME')]\n"
     ]
    }
   ],
   "source": [
    "text_example = \"Dr. Watson reviewed the patient's history before prescribing Acetaminophen 500mg daily and 5 mg of Roxicodone every 10 hours.\"\n",
    "domain_synonyms = {'Acetaminophen':'Paracetamol','Roxicodone':\"Oxycodone\"}\n",
    "rules = {'500mg': '500 mg'}\n",
    "# Applying functions\n",
    "tokenized_text = custom_tokenizer(text_example)\n",
    "vocabulary_integrated_text = integrate_domain_vocabulary(\" \".join(tokenized_text), domain_synonyms) ## controllare la formattazione\n",
    "contextually_preprocessed_text = apply_contextual_rules(vocabulary_integrated_text, rules)\n",
    "normalized_text = normalize_semantics(contextually_preprocessed_text,domain_synonyms)\n",
    "entities = recognize_entities(normalized_text, 'en_core_web_md') #Example of a domain-adapted model\n",
    "print(\"Normalized Text:\", normalized_text)\n",
    "print(\"Recognized Entities:\", entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting multiprocess\n",
      "  Downloading multiprocess-0.70.17-py311-none-any.whl.metadata (7.2 kB)\n",
      "Collecting dill>=0.3.9 (from multiprocess)\n",
      "  Downloading dill-0.3.9-py3-none-any.whl.metadata (10 kB)\n",
      "Downloading multiprocess-0.70.17-py311-none-any.whl (144 kB)\n",
      "Downloading dill-0.3.9-py3-none-any.whl (119 kB)\n",
      "Installing collected packages: dill, multiprocess\n",
      "Successfully installed dill-0.3.9 multiprocess-0.70.17\n"
     ]
    }
   ],
   "source": [
    "!pip install multiprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas\n",
      "  Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl.metadata (89 kB)\n",
      "Collecting numpy\n",
      "  Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2024.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2024.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Requirement already satisfied: six>=1.5 in /Users/vincenzo/Desktop/Libro/.venv/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Using cached pandas-2.2.3-cp311-cp311-macosx_11_0_arm64.whl (11.3 MB)\n",
      "Using cached numpy-2.1.2-cp311-cp311-macosx_14_0_arm64.whl (5.4 MB)\n",
      "Using cached pytz-2024.2-py2.py3-none-any.whl (508 kB)\n",
      "Using cached tzdata-2024.2-py2.py3-none-any.whl (346 kB)\n",
      "Installing collected packages: pytz, tzdata, numpy, pandas\n",
      "Successfully installed numpy-2.1.2 pandas-2.2.3 pytz-2024.2 tzdata-2024.2\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "#from multiprocessing import Pool\n",
    "from multiprocess import Pool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_batches(data, batch_size):\n",
    "    \"\"\"Yield successive n-sized chunks from l.\"\"\"\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        yield data[i:i + batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_batch(batch):\n",
    "    \"\"\"Process each batch - example transformation to lowercase.\"\"\"\n",
    "    return batch.str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def consolidate_results(results):\n",
    "    \"\"\"Combine processed batches into one dataset.\"\"\"\n",
    "    return pd.concat(results, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def safe_process_batch(batch):\n",
    "    \"\"\"Process batch with error handling.\"\"\"\n",
    "    try:\n",
    "        return process_batch(batch)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing batch: {e}\")\n",
    "        return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parallel_process(data, batch_size):\n",
    "    \"\"\"Use parallel processing to handle batches simultaneously.\"\"\"\n",
    "    batches = list(create_batches(data, batch_size))\n",
    "    print(batches)\n",
    "    with Pool(processes=4) as pool: # Adjust number of processes based on your system\n",
    "        results = pool.map(safe_process_batch, batches)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0    Hello WORLD\n",
      "dtype: object, 1    Batch Processing is COOL\n",
      "dtype: object, 2    Python is great for DATA processing\n",
      "dtype: object, 3    PYTHON is a powerful tool for NLP\n",
      "dtype: object]\n",
      "Processed Data:\n",
      "0                            hello world\n",
      "1               batch processing is cool\n",
      "2    python is great for data processing\n",
      "3      python is a powerful tool for nlp\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Example DataFrame\n",
    "data = pd.Series([\"Hello WORLD\", \"Batch Processing is COOL\", \"Python is great for DATA processing\",\"PYTHON is a powerful tool for NLP\"])\n",
    "\n",
    "batch_size = 1 # Processing one item at a time for illustration\n",
    "processed_results = parallel_process(data, batch_size)\n",
    "consolidated_data = consolidate_results(processed_results)\n",
    "\n",
    "print(\"Processed Data:\")\n",
    "print(consolidated_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!pip install faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore, manca questa installazione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting en-core-web-sm==3.8.0\n",
      "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.8.0/en_core_web_sm-3.8.0-py3-none-any.whl (12.8 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: en-core-web-sm\n",
      "Successfully installed en-core-web-sm-3.8.0\n",
      "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
      "You can now load the package via spacy.load('en_core_web_sm')\n"
     ]
    }
   ],
   "source": [
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from faker import Faker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_sensitive_data(text):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc = nlp(text)\n",
    "    return {ent.text: ent.label_ for ent in doc.ents if ent.label_ in ['PERSON', 'GPE', 'DATE', 'ORG']}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def anonymize_text(text, sensitive_info):\n",
    "    faker = Faker()\n",
    "    anonymized_info = {}\n",
    "    for info, label in sensitive_info.items():\n",
    "        if label == 'PERSON':\n",
    "            anonymized_info[info] = faker.name()\n",
    "        elif label == 'GPE':\n",
    "            anonymized_info[info] = faker.country()\n",
    "        elif label == 'DATE':\n",
    "            anonymized_info[info] = faker.date()\n",
    "        elif label == 'ORG':\n",
    "            anonymized_info[info] = faker.company()\n",
    "    for original, anon in anonymized_info.items():\n",
    "        text = text.replace(original, anon)\n",
    "    return text, anonymized_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_anonymization(anonymized_text, original_info,\n",
    "anonymized_info):\n",
    "    # Validation is successful if no original entity remains in the anonymized text\n",
    "    for original, _ in original_info.items():\n",
    "        if original in anonymized_text:\n",
    "            return \"Validation Failed\"\n",
    "    # Ensure that replacements are indeed different\n",
    "    for original, anon in anonymized_info.items():\n",
    "        if original == anon:\n",
    "            return \"Validation Failed\"\n",
    "    return \"Validation Successful\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_anonymization_process(info):\n",
    "    print(\"Anonymization process:\", info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## errore indentazione sbagliata pgina 29"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_re_identification_risk(anonymized_info):\n",
    "    \"\"\" Assess risks based on how distinguishable the anonymized entities are from the originals \"\"\"\n",
    "    risk_levels = []\n",
    "    for original, anon in anonymized_info.items():\n",
    "        if original.lower() == anon.lower():\n",
    "            risk_levels.append('High risk')\n",
    "        else:\n",
    "            risk_levels.append('Low risk')\n",
    "    if 'High risk' in risk_levels:\n",
    "        return \"High risk of re-identification\"\n",
    "    return \"Low risk of re-identification\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anonymization process: Validation Successful\n",
      "Original Text: Dr. John Doe works at the University of Springfield, is an associate professor of Computer Science and was born on June 6, 1965.\n",
      "Anonymized Text: Dr. Jacob Fuller works at Obrien Ltd, is an associate professor of Thornton, Fox and Gutierrez and was born on 2024-10-04.\n",
      "Validation: Validation Successful\n",
      "Risk Assessment: Low risk of re-identification\n"
     ]
    }
   ],
   "source": [
    "sample_text = \"Dr. John Doe works at the University of Springfield, is an associate professor of Computer Science and was born on June 6, 1965.\"\n",
    "sensitive_info = identify_sensitive_data(sample_text)\n",
    "anonymized_text, anonymized_info = anonymize_text(sample_text, sensitive_info)\n",
    "validation = validate_anonymization(anonymized_text, sensitive_info,anonymized_info)\n",
    "log_anonymization_process(validation)\n",
    "risk_assessment = assess_re_identification_risk(anonymized_info)\n",
    "\n",
    "print(\"Original Text:\", sample_text)\n",
    "print(\"Anonymized Text:\", anonymized_text)\n",
    "print(\"Validation:\", validation)\n",
    "print(\"Risk Assessment:\", risk_assessment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
