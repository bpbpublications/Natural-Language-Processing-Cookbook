{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install wordcloud\n",
    "!pip install matplotlib\n",
    "!pip install nltk"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Load text (you can replace this with loading from a file)\n",
    "text = \"\"\"Natural Language Processing with Python is a very interesting field. This book is meant for students and enthusiasts who are interested in learning NLP.\"\"\"\n",
    "\n",
    "# Clean the text\n",
    "text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "text = text.lower()  # Convert text to lowercase\n",
    "\n",
    "# Tokenize and remove stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "filtered_words = [word for word in text.split() if word not in stop_words]\n",
    "clean_text = ' '.join(filtered_words)\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "# Calculate word frequencies\n",
    "word_freq = Counter(filtered_words)\n",
    "\n",
    "# Create a word cloud object\n",
    "wordcloud = WordCloud(width=800, height=400, max_words=100, background_color='white', colormap='viridis')\n",
    "\n",
    "# Generate the word cloud from the word frequencies\n",
    "wordcloud.generate_from_frequencies(word_freq)\n",
    "\n",
    "# Plot the word cloud\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')  # Turn off axis\n",
    "plt.show()\n",
    "\n",
    "# Save the word cloud to a file\n",
    "wordcloud.to_file(\"wordcloud_output.png\")\n"
   ],
   "id": "b39aacce5f6fbb06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install scikit-learn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install nltk\n"
   ],
   "id": "71603e24906a5450"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "\n",
    "# Sample documents\n",
    "documents = [\n",
    "    \"Natural Language Processing enables machines to understand human language.\",\n",
    "    \"Deep learning techniques are widely used in NLP applications.\",\n",
    "    \"Natural Language Processing and Deep Learning are closely connected.\",\n",
    "    \"Support Vector Machines are another approach to machine learning.\",\n",
    "    \"Understanding human language is a key focus in AI research.\"\n",
    "]\n",
    "\n",
    "# Preprocess the documents\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "processed_docs = [preprocess(doc) for doc in documents]\n",
    "vectorizer = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer.fit_transform(processed_docs)\n",
    "similarity_matrix = cosine_similarity(tfidf_matrix)\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(similarity_matrix, annot=True, cmap='coolwarm', xticklabels=['Doc1', 'Doc2', 'Doc3', 'Doc4', 'Doc5'], \n",
    "            yticklabels=['Doc1', 'Doc2', 'Doc3', 'Doc4', 'Doc5'])\n",
    "plt.title(\"Document Similarity Heatmap\")\n",
    "plt.show()\n"
   ],
   "id": "867eb527cf3aca7d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install textblob\n",
    "!pip install nltk\n"
   ],
   "id": "5a64d09c480edbd0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from textblob import TextBlob\n",
    "import nltk\n",
    "import re\n",
    "# Example data loading (you can replace it with your dataset)\n",
    "data = pd.read_csv('sample_tweets.csv')  # Assume 'sample_tweets.csv' has 'date' and 'tweet' columns\n",
    "\n",
    "# Preprocessing function\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'http\\S+', '', text)  # Remove URLs\n",
    "    text = re.sub(r'[^A-Za-z\\s]', '', text)  # Remove punctuation and special characters\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    return text\n",
    "\n",
    "data['cleaned_tweet'] = data['tweet'].apply(preprocess_text)\n",
    "def get_sentiment(text):\n",
    "    return TextBlob(text).sentiment.polarity\n",
    "\n",
    "data['sentiment'] = data['cleaned_tweet'].apply(get_sentiment)\n",
    "# Convert 'date' column to datetime\n",
    "data['date'] = pd.to_datetime(data['date'])\n",
    "\n",
    "# Group by date and calculate mean sentiment\n",
    "sentiment_over_time = data.groupby(data['date'].dt.date)['sentiment'].mean().reset_index()\n",
    "plt.figure(figsize=(12, 6))\n",
    "sns.lineplot(x='date', y='sentiment', data=sentiment_over_time, color='blue')\n",
    "plt.title('Sentiment Timeline Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Average Sentiment Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "id": "e316ebc7c9cf90fd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!pip install matplotlib\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "id": "9954b783de197d8c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "import nltk\n",
    "from nltk.tree import Tree\n",
    "from spacy import displacy\n",
    "import matplotlib.pyplot as plt\n",
    "# Load the SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the sentence to parse\n",
    "sentence = \"The quick brown fox jumps over the lazy dog.\"\n",
    "# Parse the sentence\n",
    "doc = nlp(sentence)\n",
    "def to_nltk_tree(node):\n",
    "    if node.n_lefts + node.n_rights > 0:\n",
    "        return Tree(node.orth_, [to_nltk_tree(child) for child in node.children])\n",
    "    else:\n",
    "        return node.orth_\n",
    "\n",
    "# Convert the root token to an NLTK tree\n",
    "nltk_tree = to_nltk_tree(list(doc.sents)[0].root)\n",
    "# Visualize using NLTK\n",
    "nltk_tree.draw()\n",
    "\n",
    "# Alternative visualization using SpaCy's displacy\n",
    "displacy.render(doc, style=\"dep\", jupyter=True)\n"
   ],
   "id": "ccb423e9f18bfb75"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install gensim\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n"
   ],
   "id": "94c37034d63fc74c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import gensim.downloader as api\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "# Load the pre-trained Word2Vec model\n",
    "model = api.load(\"glove-wiki-gigaword-100\")  # GloVe embeddings with 100 dimensions\n",
    "words = ['king', 'queen', 'man', 'woman', 'apple', 'banana', 'fruit', 'orange', 'lion', 'tiger', 'cat', 'dog', 'animal']\n",
    "word_vectors = [model[word] for word in words if word in model]\n",
    "# First reduction with PCA\n",
    "pca = PCA(n_components=50)\n",
    "word_vectors_pca = pca.fit_transform(word_vectors)\n",
    "\n",
    "# Further reduction with t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=5, n_iter=500, random_state=42)\n",
    "word_vectors_2d = tsne.fit_transform(word_vectors_pca)\n",
    "embedding_df = pd.DataFrame(word_vectors_2d, columns=['x', 'y'])\n",
    "embedding_df['word'] = words\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.scatterplot(x='x', y='y', data=embedding_df, hue='word', palette='viridis')\n",
    "\n",
    "# Annotate each point with its word label\n",
    "for i in range(len(embedding_df)):\n",
    "    plt.text(embedding_df['x'][i] + 0.05, embedding_df['y'][i] + 0.05, embedding_df['word'][i], fontsize=9)\n",
    "\n",
    "plt.title('2D Projection of Word Embeddings')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.show()\n"
   ],
   "id": "63f6c72afc71b86e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install gensim\n",
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "!pip install pandas\n"
   ],
   "id": "e726a5f7c10c10b9"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from sklearn.preprocessing import normalize\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "# Load sample data\n",
    "data = pd.read_csv('documents.csv')  # Assume 'documents.csv' has a column 'text'\n",
    "\n",
    "# Preprocessing function\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in text.split() if word not in stop_words]\n",
    "    return tokens\n",
    "\n",
    "# Apply preprocessing\n",
    "data['processed_text'] = data['text'].apply(preprocess_text)\n",
    "# Create dictionary and corpus\n",
    "dictionary = corpora.Dictionary(data['processed_text'])\n",
    "corpus = [dictionary.doc2bow(text) for text in data['processed_text']]\n",
    "\n",
    "# Train LDA model\n",
    "num_topics = 5\n",
    "lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "# Extract topic distributions\n",
    "topic_distributions = []\n",
    "for doc in corpus:\n",
    "    topic_distribution = lda_model.get_document_topics(doc, minimum_probability=0)\n",
    "    topic_distributions.append([prob for _, prob in topic_distribution])\n",
    "\n",
    "# Convert to DataFrame\n",
    "topic_df = pd.DataFrame(topic_distributions, columns=[f'Topic_{i+1}' for i in range(num_topics)])\n",
    "# Normalize the topic distributions\n",
    "topic_df_normalized = pd.DataFrame(normalize(topic_df, axis=1), columns=topic_df.columns)\n",
    "topic_df_normalized['Document'] = data.index\n",
    "# Plot heatmap of topic distributions\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(topic_df_normalized.set_index('Document').T, cmap='viridis', cbar=True)\n",
    "plt.title('Heatmap of Topic Distributions Across Documents')\n",
    "plt.xlabel('Document ID')\n",
    "plt.ylabel('Topic')\n",
    "plt.show()\n",
    "\n",
    "# Plot stacked bar chart\n",
    "topic_df_normalized.set_index('Document').plot(kind='bar', stacked=True, figsize=(12, 6), colormap='Set3')\n",
    "plt.title('Stacked Bar Chart of Topic Proportions per Document')\n",
    "plt.xlabel('Document ID')\n",
    "plt.ylabel('Proportion of Topics')\n",
    "plt.show()\n"
   ],
   "id": "89a36c97b47c24c4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install spacy\n",
    "!pip install networkx\n",
    "!pip install matplotlib\n",
    "!pip install pyvis\n",
    "!python -m spacy download en_core_web_sm\n"
   ],
   "id": "906d282f095da5e8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "from pyvis.network import Network\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Define the text\n",
    "text = \"\"\"Google was founded by Larry Page and Sergey Brin while they were students at Stanford University in California. \n",
    "Elon Musk is the CEO of SpaceX and Tesla, which are based in the United States.\"\"\"\n",
    "# Extract entities and create relationships\n",
    "doc = nlp(text)\n",
    "entity_pairs = []\n",
    "\n",
    "for sent in doc.sents:\n",
    "    entities = [ent.text for ent in sent.ents]\n",
    "    if len(entities) > 1:\n",
    "        for i in range(len(entities)):\n",
    "            for j in range(i + 1, len(entities)):\n",
    "                entity_pairs.append((entities[i], entities[j]))\n",
    "# Create a graph using networkx\n",
    "G = nx.Graph()\n",
    "\n",
    "# Add nodes and edges to the graph\n",
    "for entity1, entity2 in entity_pairs:\n",
    "    G.add_node(entity1)\n",
    "    G.add_node(entity2)\n",
    "    G.add_edge(entity1, entity2)\n",
    "# Plot with Matplotlib\n",
    "plt.figure(figsize=(10, 10))\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_color='skyblue', edge_color='black', node_size=1500, font_size=10)\n",
    "plt.title('Entity Relationship Graph')\n",
    "plt.show()\n",
    "\n",
    "# Interactive visualization with Pyvis\n",
    "net = Network(notebook=True)\n",
    "net.from_nx(G)\n",
    "net.show(\"entity_relationship_graph.html\")\n"
   ],
   "id": "13bae8b6b8a3cae1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install spacy\n",
    "!pip install pandas\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!python -m spacy download en_core_web_sm\n"
   ],
   "id": "12386e99f3c05f29"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import spacy\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "import re\n",
    "# Load sample data\n",
    "data = pd.read_csv('documents.csv')  # Assume 'documents.csv' has columns 'date' and 'text'\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# Extract named entities from each document\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [ent.text for ent in doc.ents if ent.label_ in ['PERSON', 'ORG', 'GPE']]\n",
    "    return entities\n",
    "\n",
    "# Apply entity extraction\n",
    "data['entities'] = data['cleaned_text'].apply(extract_entities)\n",
    "# Count entity mentions across documents\n",
    "entity_counts = Counter()\n",
    "for entities in data['entities']:\n",
    "    entity_counts.update(entities)\n",
    "\n",
    "# Convert to DataFrame for easy manipulation\n",
    "entity_count_df = pd.DataFrame(entity_counts.items(), columns=['Entity', 'Count'])\n",
    "# Explode the list of entities for individual mentions\n",
    "data_exploded = data.explode('entities')\n",
    "\n",
    "# Aggregate entity mentions over time\n",
    "time_agg = data_exploded.groupby(['date', 'entities']).size().reset_index(name='mention_count')\n",
    "# Example: Visualize the trend of an entity over time\n",
    "entity_to_plot = 'Google'\n",
    "entity_trend = time_agg[time_agg['entities'] == entity_to_plot]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='date', y='mention_count', data=entity_trend)\n",
    "plt.title(f'Mention Trend of \"{entity_to_plot}\" Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Mention Count')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()\n",
    "\n",
    "# Example: Visualize co-occurrence of top entities using a heatmap\n",
    "top_entities = entity_count_df.nlargest(10, 'Count')['Entity']\n",
    "co_occurrence_matrix = pd.crosstab(data_exploded['date'], data_exploded['entities'])\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(co_occurrence_matrix[top_entities], cmap='YlGnBu', annot=True)\n",
    "plt.title('Heatmap of Entity Mentions Across Documents')\n",
    "plt.xlabel('Entities')\n",
    "plt.ylabel('Dates')\n",
    "plt.show()\n"
   ],
   "id": "8b8f6305a5d07a2b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install nltk"
   ],
   "id": "f81c672fc1445e89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_curve, auc, precision_recall_curve\n",
    "import nltk\n",
    "import re\n",
    "# Load dataset (for example, a CSV file with columns 'text' and 'label')\n",
    "data = pd.read_csv('text_classification_data.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    return text\n",
    "\n",
    "# Apply preprocessing\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "# Convert text to numerical format\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Define labels\n",
    "y = data['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Train a Naive Bayes classifier\n",
    "classifier = MultinomialNB()\n",
    "classifier.fit(X_train, y_train)\n",
    "# Predict labels for the test set\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "# Generate classification report\n",
    "print(classification_report(y_test, y_pred))\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Plot confusion matrix using Seaborn\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=classifier.classes_, yticklabels=classifier.classes_)\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix')\n",
    "plt.show()\n",
    "# Generate precision-recall curve\n",
    "precision, recall, _ = precision_recall_curve(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(recall, precision, color='b')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.show()\n",
    "\n",
    "# Generate ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, classifier.predict_proba(X_test)[:, 1])\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(fpr, tpr, color='r', label=f'ROC Curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='gray', linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ],
   "id": "722c0ccb5b37c831"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install nltk\n",
    "!pip install wordcloud\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install pandas\n"
   ],
   "id": "3c6edba4c0d61953"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "# Load sample text data (for example, a CSV file with a column 'text')\n",
    "data = pd.read_csv('text_data.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "# Calculate word frequencies\n",
    "word_freq = pd.Series(' '.join(data['cleaned_text']).split()).value_counts()\n",
    "\n",
    "# Perform sentiment analysis (for demonstration, we'll assign random values as sentiment scores)\n",
    "import numpy as np\n",
    "data['sentiment'] = np.random.uniform(-1, 1, len(data))  # Random sentiment scores for illustration\n",
    "# Generate a word cloud\n",
    "wordcloud = WordCloud(width=800, height=400, background_color='white').generate(' '.join(data['cleaned_text']))\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.imshow(wordcloud, interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.title('Word Cloud of Most Frequent Words')\n",
    "plt.show()\n",
    "\n",
    "# Bar chart for top 10 word frequencies\n",
    "top_words = word_freq[:10]\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x=top_words.values, y=top_words.index, palette='viridis')\n",
    "plt.title('Top 10 Most Frequent Words')\n",
    "plt.xlabel('Frequency')\n",
    "plt.ylabel('Words')\n",
    "plt.show()\n",
    "\n",
    "# Line chart for sentiment over time\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(data.index, data['sentiment'], color='blue', linestyle='-', marker='o')\n",
    "plt.title('Sentiment Trend Over Time')\n",
    "plt.xlabel('Document Index')\n",
    "plt.ylabel('Sentiment Score')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "# Create a combined infographic with subplots\n",
    "fig, axs = plt.subplots(2, 2, figsize=(18, 12))\n",
    "\n",
    "# Word Cloud\n",
    "axs[0, 0].imshow(wordcloud, interpolation='bilinear')\n",
    "axs[0, 0].axis('off')\n",
    "axs[0, 0].set_title('Word Cloud of Most Frequent Words')\n",
    "\n",
    "# Bar Chart\n",
    "sns.barplot(x=top_words.values, y=top_words.index, palette='viridis', ax=axs[0, 1])\n",
    "axs[0, 1].set_title('Top 10 Most Frequent Words')\n",
    "axs[0, 1].set_xlabel('Frequency')\n",
    "axs[0, 1].set_ylabel('Words')\n",
    "\n",
    "# Line Chart for Sentiment Trend\n",
    "axs[1, 0].plot(data.index, data['sentiment'], color='blue', linestyle='-', marker='o')\n",
    "axs[1, 0].set_title('Sentiment Trend Over Time')\n",
    "axs[1, 0].set_xlabel('Document Index')\n",
    "axs[1, 0].set_ylabel('Sentiment Score')\n",
    "axs[1, 0].grid(True)\n",
    "\n",
    "# Placeholder for future elements (e.g., pie chart or other summary statistics)\n",
    "axs[1, 1].text(0.5, 0.5, 'Additional Summary or Visualization Here', \n",
    "               horizontalalignment='center', verticalalignment='center', fontsize=14)\n",
    "axs[1, 1].set_axis_off()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "4d285bdabb807d2f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install sklearn\n",
    "!pip install matplotlib\n",
    "!pip install seaborn\n",
    "!pip install nltk\n",
    "!pip install umap-learn\n",
    "!pip install gensim\n"
   ],
   "id": "1ea39a9c3c4ad0cf"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "import nltk\n",
    "import re\n",
    "from gensim.models import Word2Vec\n",
    "# Load dataset (for example, a CSV file with a column 'text')\n",
    "data = pd.read_csv('text_data.csv')\n",
    "\n",
    "# Preprocessing function\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = re.sub(r'\\d+', '', text)  # Remove numbers\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove punctuation\n",
    "    text = text.lower()  # Convert to lowercase\n",
    "    tokens = [word for word in text.split() if word not in stop_words]\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing\n",
    "data['cleaned_text'] = data['text'].apply(preprocess_text)\n",
    "# Convert text to TF-IDF vectors\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_tfidf = vectorizer.fit_transform(data['cleaned_text'])\n",
    "\n",
    "# Alternatively, generate Word2Vec embeddings\n",
    "sentences = [text.split() for text in data['cleaned_text']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, workers=4)\n",
    "X_word2vec = np.array([np.mean([word2vec_model.wv[word] for word in sentence if word in word2vec_model.wv]\n",
    "                                or [np.zeros(100)], axis=0) for sentence in sentences])\n",
    "# Initial dimensionality reduction with PCA\n",
    "pca = PCA(n_components=50)\n",
    "X_pca = pca.fit_transform(X_tfidf.toarray())\n",
    "\n",
    "# Further reduction with t-SNE\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=500, random_state=42)\n",
    "X_tsne = tsne.fit_transform(X_pca)\n",
    "\n",
    "# Alternatively, use UMAP for reduction\n",
    "umap_model = umap.UMAP(n_neighbors=15, n_components=2, random_state=42)\n",
    "X_umap = umap_model.fit_transform(X_pca)\n",
    "# Create DataFrame for visualization\n",
    "df_tsne = pd.DataFrame(X_tsne, columns=['Dimension 1', 'Dimension 2'])\n",
    "df_tsne['label'] = data['label']  # Assume there's a 'label' column for categorization\n",
    "\n",
    "df_umap = pd.DataFrame(X_umap, columns=['Dimension 1', 'Dimension 2'])\n",
    "df_umap['label'] = data['label']\n",
    "# Visualize t-SNE result\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='label', palette='viridis', data=df_tsne, s=60, alpha=0.7)\n",
    "plt.title('t-SNE Visualization of Text Data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend(title='Category')\n",
    "plt.show()\n",
    "\n",
    "# Visualize UMAP result\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x='Dimension 1', y='Dimension 2', hue='label', palette='coolwarm', data=df_umap, s=60, alpha=0.7)\n",
    "plt.title('UMAP Visualization of Text Data')\n",
    "plt.xlabel('Dimension 1')\n",
    "plt.ylabel('Dimension 2')\n",
    "plt.legend(title='Category')\n",
    "plt.show()"
   ],
   "id": "1dfcba6d5092c1cb"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "2a0bfc443e5974e1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "38880cc40b0800cc"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
