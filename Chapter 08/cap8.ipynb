{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install pytorch"
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "from transformers import pipeline\n",
    "nltk.download('punkt')"
   ],
   "id": "61e28f9aa07ac49f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "conversational_pipeline = pipeline('text-generation',\n",
    "model='gpt2',framework=\"pt\")\n",
    "def tokenize_and_understand(text):\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def manage_dialogue(conversation_history):\n",
    "    return conversation_history[-1] if conversation_history else\"\"\n",
    "\n",
    "def generate_response(user_input):\n",
    "    response = conversational_pipeline(user_input, max_length=50,num_return_sequences=1)\n",
    "    return response[0]['generated_text']\n",
    "\n",
    "def chatbot_interface():\n",
    "    print(\"Welcome to the Chatbot! Type 'exit' to end the conversation.\")\n",
    "    conversation_history = []\n",
    "    while True:\n",
    "        user_input = input(\"You: \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            print(\"Chatbot: Goodbye!\")\n",
    "            break\n",
    "        tokens = tokenize_and_understand(user_input)\n",
    "        conversation_history.append(user_input)\n",
    "        current_context = manage_dialogue(conversation_history)\n",
    "        response = generate_response(current_context)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "chatbot_interface()"
   ],
   "id": "ac903be17d6f5a4"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install scikit-learn\n",
    "!pip install spacy\n",
    "!pip install nltk\n",
    "!python -m spacy download en_core_web_sm"
   ],
   "id": "786f70c7e2d2b524"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "nltk.download('punkt')\n",
    "data = [\n",
    " (\"What's the weather like today?\", \"weather\"),\n",
    " (\"Tell me a joke\", \"joke\"),\n",
    " (\"What's the time?\", \"time\"),\n",
    " (\"How's the stock market doing?\", \"stock_market\"),\n",
    " (\"Book a flight to New York\", \"book_flight\"),\n",
    " (\"Play some music\", \"play_music\")\n",
    "]\n",
    "inputs, labels = zip(*data)\n",
    "X_train, X_test, y_train, y_test = train_test_split(inputs, labels,\n",
    "                                                    test_size=0.2, random_state=42)\n",
    "vectorizer = TfidfVectorizer(tokenizer=nltk.word_tokenize)\n",
    "X_train_tfidf = vectorizer.fit_transform(X_train)\n",
    "X_test_tfidf = vectorizer.transform(X_test)\n",
    "model = LogisticRegression()\n",
    "model.fit(X_train_tfidf, y_train)\n",
    "y_pred = model.predict(X_test_tfidf)\n",
    "# Print the classification report to evaluate the model\n",
    "print(classification_report(y_test, y_pred))\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def recognize_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(ent.text, ent.label_) for ent in doc.ents]\n",
    "    return entities\n",
    "\n",
    "text = \"Book a flight to New York on September 20th\"\n",
    "entities = recognize_entities(text)\n",
    "print(f\"Entities in '{text}': {entities}\")"
   ],
   "id": "5a12f35bfda3f3f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install nltk\n",
    "!pip install transformers\n",
    "!pip install json"
   ],
   "id": "b76700e7c7785b28"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-15T21:16:11.709340Z",
     "start_time": "2025-01-15T21:16:11.705090Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from transformers import pipeline\n",
    "import json\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "# Define states and transitions for the dialog\n",
    "states = {\n",
    " \"greeting\": {\n",
    "     \"responses\": [\"Hello! How can I help you today?\", \"Hi there!What can I do for you?\"],\n",
    "     \"transitions\": [\"request_info\", \"end_conversation\"]},\n",
    "\"request_info\": {\n",
    "    \"responses\": [\"Sure, I can help with that. What do you need?\",\n",
    "\"What information are you looking for?\"], \n",
    "    \"transitions\": [\"provide_info\", \"end_conversation\"] }, \n",
    "    \"provide_info\": { \"responses\": [\"Here is the information you requested.\", \"This iswhat I found for you.\"],\n",
    " \"transitions\": [\"end_conversation\"]},\n",
    " \"end_conversation\": {\n",
    " \"responses\": [\"Goodbye!\", \"Have a great day!\"],\n",
    " \"transitions\": []\n",
    " }\n",
    "}\n",
    "\n",
    "current_state = \"greeting\"\n",
    "context = {}\n",
    "def get_next_state(user_input, current_state):\n",
    " # Logic to determine the next state based on user input\n",
    "    if \"info\" in user_input.lower():\n",
    "        return \"request_info\"\n",
    "    elif \"bye\" in user_input.lower():\n",
    "        return \"end_conversation\"\n",
    "    else:\n",
    "        return current_state\n",
    "\n",
    "def generate_response(state):\n",
    "    responses = states[state][\"responses\"]\n",
    "    return responses[0]\n",
    "\n",
    "def chatbot():\n",
    "    global current_state\n",
    "    print(\"Chatbot: Hello! How can I help you today?\")\n",
    "\n",
    "    while current_state != \"end_conversation\":\n",
    "        user_input = input(\"You: \")\n",
    "        next_state = get_next_state(user_input, current_state)\n",
    "        response = generate_response(next_state)\n",
    "        print(f\"Chatbot: {response}\")\n",
    "        current_state = next_state\n",
    "# Run the chatbot\n",
    "chatbot()"
   ],
   "id": "997ab6199e2de920",
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (3168814406.py, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;36m  Cell \u001B[0;32mIn[1], line 7\u001B[0;36m\u001B[0m\n\u001B[0;31m    10.states = {\u001B[0m\n\u001B[0m      ^\u001B[0m\n\u001B[0;31mSyntaxError\u001B[0m\u001B[0;31m:\u001B[0m invalid decimal literal\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install nltk                                     ",
   "id": "dc5a92a3558ee1c8"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "import json\n",
    "nltk.download('punkt')\n",
    "def load_user_profiles(file_path):\n",
    "    try:\n",
    "        with open(file_path, 'r') as file:\n",
    "            return json.load(file)\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "def save_user_profiles(file_path, profiles):\n",
    "    with open(file_path, 'w') as file:\n",
    "        json.dump(profiles, file)\n",
    "        profiles = load_user_profiles('user_profiles.json')\n",
    "        \n",
    "def update_context(user_id, user_input, context):\n",
    "    context[user_id] = user_input\n",
    "    return context\n",
    "\n",
    "def generate_response(user_id, profiles, context):\n",
    "    user_profile = profiles.get(user_id, {})\n",
    "    last_context = context.get(user_id,\"\")\n",
    "    if \"name\" in user_profile:\n",
    "        return f\"Hello {user_profile['name']}! You said:{last_context}. How can I assist you further?\"\n",
    "    else:\n",
    "        return \"Hello! How can I assist you today?\"\n",
    "    \n",
    "def learn_preferences(user_id, user_input, profiles):\n",
    " if user_id not in profiles:\n",
    "    profiles[user_id] = {}\n",
    "    profiles[user_id]['last_input'] = user_input\n",
    " return profiles\n",
    "def custom_greeting(user_id, profiles):\n",
    "    user_profile = profiles.get(user_id, {})\n",
    "    if \"name\" in user_profile:\n",
    "        return f\"Welcome back, {user_profile['name']}!\"\n",
    "    else:\n",
    "        return \"Welcome! How can I help you today?\"\n",
    "def custom_farewell(user_id, profiles):\n",
    "    user_profile = profiles.get(user_id, {})\n",
    "    if \"name\" in user_profile:\n",
    "        return f\"Goodbye, {user_profile['name']}! Have a great day!\"\n",
    "    else:\n",
    "        return \"Goodbye! Have a great day!\""
   ],
   "id": "f2e2e2111f8e05dc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install requests",
   "id": "49593c811f8e17f1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import requests\n",
    "import json\n",
    "API_URL = \"https://api.openweathermap.org/data/2.5/weather\"\n",
    "API_KEY = \"your_api_key_here\"\n",
    "\n",
    "def get_weather_data(city):\n",
    "    params = {\n",
    "        'q': city,\n",
    "        'appid': API_KEY,\n",
    "        'units': 'metric'\n",
    "    }\n",
    "    response = requests.get(API_URL, params=params)\n",
    "    return response.json()\n",
    "\n",
    "def fetch_weather(city):\n",
    "    data = get_weather_data(city)\n",
    "    if data['cod'] == 200:\n",
    "        return data\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_weather_data(data):\n",
    "    city = data['name']\n",
    "    temperature = data['main']['temp']\n",
    "    weather_description = data['weather'][0]['description']\n",
    "    return f\"The weather in {city} is currently {weather_description} with a temperature of {temperature}°C.\"\n",
    "\n",
    "def get_weather(city):\n",
    "    try:\n",
    "        data = fetch_weather(city)\n",
    "        if data:\n",
    "            return parse_weather_data(data)\n",
    "        else:\n",
    "            return \"Sorry, I couldn't find the weather information for that location.\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\""
   ],
   "id": "98912c6c99b17ee5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install nltk",
   "id": "81e2bcb012e59d65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "import json\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "def start_session(user_id):\n",
    "    return {\"user_id\": user_id, \"context\": {}, \"state\": \"initial\"}\n",
    "\n",
    "def get_session(user_id, sessions):\n",
    "    return sessions.get(user_id, start_session(user_id))\n",
    "\n",
    "def update_state(session, new_state):\n",
    "    session[\"state\"] = new_state\n",
    "    return session\n",
    "\n",
    "def store_context(session, key, value):\n",
    "    session[\"context\"][key] = value\n",
    "    return session\n",
    "\n",
    "def retrieve_context(session, key):\n",
    "    return session[\"context\"].get(key, None)\n",
    "\n",
    "def update_context(session, user_input):\n",
    "    tokens = nltk.word_tokenize(user_input)\n",
    "    if \"name\" in tokens:\n",
    "        name = tokens[tokens.index(\"name\") + 1]\n",
    "        session = store_context(session, \"name\", name)\n",
    "    return session\n"
   ],
   "id": "106ed52b6b6a66b1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install nltk sklearn json",
   "id": "7107f286a9c28014"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import nltk\n",
    "import json\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "\n",
    "def evaluate_metrics(true_labels, predicted_labels):\n",
    "    report = classification_report(true_labels, predicted_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    return report, accuracy\n",
    "\n",
    "def collect_feedback(conversation_log):\n",
    "    feedback = {}\n",
    "    for entry in conversation_log:\n",
    "        feedback[entry['user_id']] = entry['feedback']\n",
    "    return feedback\n",
    "\n",
    "def automated_testing(test_cases, chatbot_response_function):\n",
    "    results = []\n",
    "    for case in test_cases:\n",
    "        user_input = case['input']\n",
    "        expected_output = case['expected_output']\n",
    "        actual_output = chatbot_response_function(user_input)\n",
    "        results.append((expected_output, actual_output))\n",
    "    return results\n",
    "\n",
    "def ab_testing(version_a, version_b, test_cases):\n",
    "    results_a = automated_testing(test_cases, version_a)\n",
    "    results_b = automated_testing(test_cases, version_b)\n",
    "    return results_a, results_b\n",
    "\n",
    "def analyze_logs(log_file):\n",
    "    with open(log_file, 'r') as file:\n",
    "        logs = json.load(file)\n",
    "    issues = []\n",
    "    for log in logs:\n",
    "        if log['error']:\n",
    "            issues.append(log)\n",
    "    return issues\n"
   ],
   "id": "ffed33a8c1901b89"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install cryptography\n",
    "!pip install jwt\n",
    "!pip install flask\n",
    "from cryptography.fernet import Fernet\n",
    "# Generate a key for encryption\n",
    "key = Fernet.generate_key()\n",
    "cipher_suite = Fernet(key)\n",
    "message = \"Sensitive conversation data\"\n",
    "encrypted_message = cipher_suite.encrypt(message.encode())\n",
    "print(f'Encrypted: {encrypted_message}')\n",
    "# Decrypting the message\n",
    "decrypted_message = cipher_suite.decrypt(encrypted_message).decode()\n",
    "print(f'Decrypted: {decrypted_message}')\n",
    "\n",
    "import jwt\n",
    "import datetime\n",
    "# Secret key for JWT\n",
    "secret_key = \"your_secret_key\"\n",
    "# Creating a JWT token\n",
    "def create_token(user_id):\n",
    "    payload = {\n",
    "     \"user_id\": user_id,\n",
    "     \"exp\": datetime.datetime.utcnow() + datetime.timedelta(hours=1)\n",
    "     }\n",
    "    token = jwt.encode(payload, secret_key, algorithm=\"HS256\")\n",
    "    return token\n",
    "# Verifying a JWT token\n",
    "def verify_token(token):\n",
    "    try:\n",
    "        payload = jwt.decode(token, secret_key, algorithms=[\"HS256\"])\n",
    "        return payload[\"user_id\"]\n",
    "    except jwt.ExpiredSignatureError:\n",
    "        return \"Token has expired\"\n",
    "    except jwt.InvalidTokenError:\n",
    "        return \"Invalid token\"\n",
    "# Example usage\n",
    "token = create_token(\"user123\")\n",
    "print(f'Token: {token}')\n",
    "user_id = verify_token(token)\n",
    "print(f'User ID: {user_id}')\n",
    "\n",
    "import pandas as pd\n",
    "data = {\"user_id\": [\"user123\", \"user456\"],\"conversation\": [\"Hello, my name is John\", \"Can you help me with my account number 987654\"]}\n",
    "df = pd.DataFrame(data)\n",
    "df[\"conversation\"] = df[\"conversation\"].str.replace(r'\\b\\d{6}\\b','XXXXXX')\n",
    "df[\"conversation\"] = df[\"conversation\"].str.replace(r'\\b\\w+?\\b', 'Anon',\n",
    "n=1)\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(filename='chatbot.log', level=logging.INFO)\n",
    "def log_conversation(user_id, message):\n",
    "    logging.info(f\"User {user_id}: {message}\")\n",
    "\n",
    "log_conversation(\"user123\", \"This is a logged message\")\n",
    "\n",
    "from flask_limiter import Limiter\n",
    "from flask import Flask, request\n",
    "app = Flask(__name__)\n",
    "limiter = Limiter(app, key_func=lambda: request.remote_addr)\n",
    "@app.route(\"/chat\", methods=[\"POST\"])\n",
    "@limiter.limit(\"5 per minute\")\n",
    "def chat():\n",
    "    return \"Chat response\"\n",
    "if __name__ == \"__main__\":\n",
    "    app.run()\n",
    "\n",
    "def secure_function(input_data):\n",
    "    if not isinstance(input_data, str) or any(char in input_data for char in [';','--']):\n",
    "        raise ValueError(\"Invalid input\")\n",
    "        # Process the input securely\n",
    "    return f\"Processed {input_data}\"\n",
    "\n",
    "try:\n",
    "    result = secure_function(\"safe_input\")\n",
    "    print(result)\n",
    "except ValueError as e:\n",
    "    print(e)\n"
   ],
   "id": "b6c0a30e1fe80941"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install langdetect\n",
    "!pip install googletrans==4.0.0-rc1"
   ],
   "id": "3c8b1167d0a5b9e1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "from langdetect import detect\n",
    "# Detecting language of a sample text\n",
    "text = \"Bonjour, comment puis-je vous aider?\"\n",
    "detected_language = detect(text)\n",
    "print(f'Detected Language: {detected_language}')\n",
    "\n",
    "from googletrans import Translator\n",
    "translator = Translator()\n",
    "# Translating a text from French to English\n",
    "text = \"Bonjour, comment puis-je vous aider?\"\n",
    "translated_text = translator.translate(text, src='fr', dest='en').text\n",
    "print(f'Translated Text: {translated_text}')\n",
    "\n",
    "responses = {\n",
    "\"en\": {\"greeting\": \"Hello, how can I help you?\"},\n",
    "\"fr\": {\"greeting\": \"Bonjour, comment puis-je vous aider?\"}}\n",
    "# Fetching the response based on detected language\n",
    "user_language = \"fr\"\n",
    "response = responses.get(user_language, responses[\"en\"])[\"greeting\"]\n",
    "print(f'Response: {response}')\n",
    "\n",
    "cultural_responses = {\n",
    "\"en\": {\"holiday_greeting\": \"Happy Holidays!\"},\n",
    "\"fr\": {\"holiday_greeting\": \"Joyeuses Fêtes!\"}}\n",
    "# Fetching the culturally adapted response\n",
    "user_language = \"fr\"\n",
    "holiday_greeting = cultural_responses.get(user_language,\n",
    "cultural_responses[\"en\"])[\"holiday_greeting\"]\n",
    "print(f'Holiday Greeting: {holiday_greeting}')\n",
    "\n",
    "ui_elements = {\n",
    " \"en\": {\"button_text\": \"Submit\"},\n",
    " \"fr\": {\"button_text\": \"Soumettre\"}\n",
    "}\n",
    "# Fetching the localized UI element\n",
    "user_language = \"fr\"\n",
    "button_text = ui_elements.get(user_language,ui_elements[\"en\"])[\"button_text\"]\n",
    "print(f'Button Text: {button_text}')\n",
    "\n",
    "def qa_check(text, src_lang, dest_lang):\n",
    "    translation = translator.translate(text, src=src_lang,dest=dest_lang).text\n",
    "    back_translation = translator.translate(translation, src=dest_lang,dest=src_lang).text\n",
    "    print(f'Original: {text}')\n",
    "    print(f'Translation: {translation}')\n",
    "    print(f'Back Translation: {back_translation}')\n",
    "    return text == back_translation\n",
    "# Performing QA check on a sample text\n",
    "qa_pass = qa_check(\"Bonjour, comment puis-je vous aider?\", 'fr', 'en')\n",
    "print(f'QA Check Passed: {qa_pass}')"
   ],
   "id": "6b35ecd32772fba3"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "!pip install scikit-learn\n",
    "!pip install pandas\n",
    "!pip install numpy\n",
    "!pip install nltk"
   ],
   "id": "30361df384616fcd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "# Sample data collection\n",
    "data = {'questions': [\"What is your name?\", \"How are you?\", \"What can you do?\", \"Tell me a joke.\"],\n",
    "        'responses': [\"I am a chatbot.\", \"I am doing great, thank you!\", \"I can assist you with various tasks.\", \"Why did the scarecrow win an award? Because he was outstanding in his field!\"]}\n",
    "df = pd.DataFrame(data)\n",
    "# Preprocessing: Tokenization and removing stopwords\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "def preprocess(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    filtered_tokens = [w for w in tokens if w.lower() not in stop_words]\n",
    "    return' '.join(filtered_tokens)\n",
    "df['processed_questions'] = df['questions'].apply(preprocess)\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(df['processed_questions'])\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "# Selecting Naive Bayes classifier for this example\n",
    "model = MultinomialNB()\n",
    "\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y = label_encoder.fit_transform(df['responses'])\n",
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,\n",
    "random_state=42)\n",
    "# Training the model\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predicting on the test set\n",
    "y_pred = model.predict(X_test)\n",
    "# Evaluating the model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "report = classification_report(y_test, y_pred,target_names=label_encoder.classes_)\n",
    "print(f'Accuracy: {accuracy}')\n",
    "print(f'Classification Report:\\n{report}')\n",
    "\n",
    "def get_chatbot_response(question):\n",
    "    processed_question = preprocess(question)\n",
    "    question_vector = vectorizer.transform([processed_question])\n",
    "    predicted_response_index = model.predict(question_vector)\n",
    "    response = label_encoder.inverse_transform(predicted_response_index)\n",
    "    return response[0]\n",
    "user_input = \"Can you tell me a joke?\"\n",
    "chatbot_response = get_chatbot_response(user_input)\n",
    "print(f'User: {user_input}')\n",
    "print(f'Chatbot: {chatbot_response}')"
   ],
   "id": "508a13abac8bf2c6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "1. !pip install nltk\n",
    "2. !pip install textblob\n",
    "3. !pip install transformers"
   ],
   "id": "c1a6bf33cad98ed5"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "personality_traits = {\n",
    "'name': 'ChatBot',\n",
    "'tone': 'friendly',\n",
    "'style': 'informal',\n",
    "'traits': ['helpful', 'humorous', 'empathetic']\n",
    "}\n",
    "persona = {\n",
    " 'background': 'I am ChatBot, your virtual assistant.',\n",
    " 'preferences': {\n",
    " 'hobbies': ['reading', 'joking', 'helping people'],\n",
    " 'favorite_color': 'blue',\n",
    " 'favorite_food': 'pizza'\n",
    " },\n",
    " 'behavior': {\n",
    " 'greeting': 'Hey there! How can I assist you today?',\n",
    " 'farewell': 'Goodbye! Have a great day!'\n",
    " }\n",
    "}\n",
    "def greet_user(user_name):\n",
    "    return f\"Hey {user_name}! It's great to see you. How can I help you today?\"\n",
    "user_name = \"John\"\n",
    "greeting = greet_user(user_name)\n",
    "print(f\"Greeting: {greeting}\")\n",
    "\n",
    "from textblob import TextBlob\n",
    "# Example sentiment analysis\n",
    "def analyze_sentiment(user_input):\n",
    "    analysis = TextBlob(user_input)\n",
    "    if analysis.sentiment.polarity > 0:\n",
    "        return \"positive\"\n",
    "    elif analysis.sentiment.polarity < 0:\n",
    "        return \"negative\"\n",
    "    else:\n",
    "        return \"neutral\"\n",
    "user_input = \"I had a terrible day.\"\n",
    "sentiment = analyze_sentiment(user_input)\n",
    "print(f\"User Sentiment: {sentiment}\")\n",
    "\n",
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='gpt-2')\n",
    "# Example response generation\n",
    "def generate_response(prompt):\n",
    "    response = generator(prompt, max_length=50, num_return_sequences=1)\n",
    "    return response[0]['generated_text']\n",
    "prompt = \"Tell me a joke\"\n",
    "response = generate_response(prompt)\n",
    "print(f\"Generated Response: {response}\")\n",
    "def humorous_response(user_input):\n",
    "    jokes = [\n",
    " \"Why don’t scientists trust atoms? Because they make up everything!\",\n",
    " \"Why did the scarecrow win an award? Because he was outstanding in his field!\"\n",
    " ]\n",
    "    return jokes[0] # You can randomize or choose based on context\n",
    "user_input = \"Tell me something funny.\"\n",
    "funny_response = humorous_response(user_input)\n",
    "print(f\"Humorous Response: {funny_response}\")\n",
    "# Example empathetic response\n",
    "def empathetic_response(user_input):\n",
    "    return \"I'm really sorry to hear that. I'm here for you.\"\n",
    "user_input = \"I feel sad today.\"\n",
    "empathy_response = empathetic_response(user_input)\n",
    "print(f\"Empathetic Response: {empathy_response}\")"
   ],
   "id": "e764b2057170a315"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "70220aef2bdafde8"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
